/**
 * Summary:
 * 	 Improved floating point forward propagation of 1 linear layer.
 */

package forward;

import utils.DotProductKernel;

import com.maxeler.maxcompiler.v2.kernelcompiler.Kernel;
import com.maxeler.maxcompiler.v2.kernelcompiler.KernelParameters;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count.Counter;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count.Params;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count.WrapMode;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.CounterChain;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Mem.RamWriteMode;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Stream.OffsetExpr;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.memory.Memory;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.KernelMath;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEFix.SignMode;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEType;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEVar;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVector;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVectorType;
import com.maxeler.maxcompiler.v2.utils.MathUtils;

public class FLinearLayerFloatKernel extends Kernel {

  public static final String IN_NAME = "input";
  public static final String W_NAME = "weights";
  public static final String B_NAME = "biases";
  public static final String S_NAME = "s";
  public static final String X_NAME = "x";
  public static final String INNBVEC_NAME = "innbvec";
  public static final String OUTNBVEC_NAME = "outnbvec";
  public static final String OFFSET = "offset";

  private final DFEVectorType<DFEVar>  parallelVec, outVec;

  public FLinearLayerFloatKernel(KernelParameters p, int inVecSize, int oVecSize, int nbWeightVec, int nbBiases, int nbInVec, int nbOutVec) {
    super(p);

    this.parallelVec = new DFEVectorType<DFEVar>(dfeFloat(8,24), inVecSize);
    this.outVec = new DFEVectorType<DFEVar>(dfeFloat(8,24), oVecSize);

    DFEVar nbVecI = io.scalarInput(INNBVEC_NAME, dfeUInt(MathUtils.bitsToAddress(nbInVec+1))); 
    DFEVar nbVecO = io.scalarInput(OUTNBVEC_NAME, dfeUInt(MathUtils.bitsToAddress(nbOutVec+1))); 

    OffsetExpr loopLatency = stream.makeOffsetAutoLoop(OFFSET);
    DFEVar loopLatencyVal = loopLatency.getDFEVar(getKernel(), dfeUInt(32));

    Params wCounterParams = control.count.makeParams (MathUtils.bitsToAddress(nbWeightVec) + 1).withMax(nbWeightVec).withWrapMode(WrapMode.STOP_AT_MAX);
    Counter wCounter = control.count.makeCounter(wCounterParams);
    DFEVar readingW = wCounter.getCount() < nbWeightVec ? constant.var(true) : constant.var(false);
    DFEVar readingB = wCounter.getCount() < nbBiases ? constant.var(true) : constant.var(false);

    CounterChain chain = control.count.makeCounterChain(~readingW);
    DFEVar h = chain.addCounter(nbVecO, 1);
    DFEVar w = chain.addCounter(nbVecI, 1);
    DFEVar l = chain.addCounter(loopLatencyVal, 1);

    DFEVector<DFEVar> input = io.input(IN_NAME, parallelVec, ~readingW & h.eq(0) & l.eq(0));
    DFEVector<DFEVar> weights = io.input(W_NAME, parallelVec, readingW);
    DFEVar biases = io.input(B_NAME, dfeFloat(8,24), readingB);
    DFEVector<DFEVar> s = outVec.newInstance(getKernel());
    DFEVector<DFEVar> x = outVec.newInstance(getKernel());

    // Write weights and biases to the memory
    Memory<DFEVector<DFEVar>> wMem = mem.alloc(parallelVec, nbWeightVec);
    Memory<DFEVar> bMem = mem.alloc(dfeFloat(8,24), nbBiases);
    wMem.write(wCounter.getCount().cast(dfeUInt(MathUtils.bitsToAddress(nbWeightVec))), weights, readingW);
    bMem.write(wCounter.getCount().cast(dfeUInt(MathUtils.bitsToAddress(nbBiases))), biases, readingB);

    // Write each input vector to the memory (multiple dot product with diff weights)
    Memory<DFEVector<DFEVar>> ibuf = mem.alloc(parallelVec, nbInVec+1);
    DFEVector<DFEVar> inPort = ibuf.port(w, input, h.eq(0), RamWriteMode.WRITE_FIRST);

    DFEVar wcount = h.cast(dfeUInt(32)) * nbVecI.cast(dfeUInt(32)) * oVecSize + w.cast(dfeUInt(32));

    DFEVar address = dfeUInt(MathUtils.bitsToAddress(nbWeightVec)).newInstance(getKernel());
    DFEVector<DFEVar> coeff = parallelVec.newInstance(getKernel());
    DFEVar tmp = dfeFloat(8,24).newInstance(getKernel());
    DFEVar newSum = dfeFloat(8,24).newInstance(getKernel());
    DFEVar bias = dfeFloat(8,24).newInstance(getKernel());

    // Compute oVecSize dot products at each clock cycle
    for (int r = 0; r < oVecSize; r++) {
      address = wcount + r*nbVecI.cast(dfeUInt(32));

      // Read correct weight chunk in memory
      coeff = wMem.read(address.cast(dfeUInt(MathUtils.bitsToAddress(nbWeightVec))));

      // Perform dot product between input and weights vector
      DotProductKernel dp = new DotProductKernel(getKernel(), inVecSize, dfeFloat(8,24));
      dp.setInputs(inPort, coeff);
      tmp = dp.getOutput();

      // Add all corresponding dot products together
      DFEVar carriedSum = dfeFloat(8,24).newInstance(this);
      DFEVar sum = (w.eq(0)) ? constant.var(0).cast(dfeFloat(8,24)) : carriedSum;
      newSum = sum + tmp;	
      carriedSum.connect(stream.offset(newSum, -loopLatency));

      // Add corresponding bias and apply activation function
      DFEVar biasAddress = h.cast(dfeUInt(32))*oVecSize + r;
      bias = bMem.read(biasAddress.cast(dfeUInt(MathUtils.bitsToAddress(nbBiases))));
      s[r] <== newSum + bias;
      x[r] <== tanh(s[r]);	
    }

    // Only enable output when all operations have been done for 1 input vector
    DFEVar outEnable = w.eq(nbVecI - 1) & l.eq(loopLatencyVal - 1);	

    io.output(S_NAME, outVec, outEnable).connect(s);
    io.output(X_NAME, outVec, outEnable).connect(x);
  }

  public DFEVar tanh(DFEVar input) {
    DFEVar x = input;
    // Exp
    DFEVar Exp2xPlus1 = KernelMath.exp(2*x, dfeFloat(8,24)) + 1.0;
    // Div
    DFEVar DivResult = 2.0 / Exp2xPlus1;
    // Sub
    DFEVar Result = 1.0 - DivResult.cast(dfeFloat(8,24));
    return Result;
  }
}

