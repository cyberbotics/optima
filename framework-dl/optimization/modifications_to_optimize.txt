* for neuron and layer : big number of member access inside net : create local variable to access in forward and backward propagation (current_neuron)
* size() vector calls: too big number first : add nb weights member in neuron class: this way no need to recalculate length of vector each time
* [] vector access operator can be optimized with O3. 
* O3 also optimizes loops which are a big part of the bottleneck of backpropagation + forwardpropagation
* O3 also optimizes some other parts of code


Tests made with 3 epoch on single layer of size 50.

STEPS:
* compile with g++ -O3 -g -I/mnist DNN-Framework.cpp -o DNN-Framework -pg
* run executable file once
* gprof executable >location/to/txt/results

NOW:
* Bottlenecks are forward and back prop + MNIST load -> FPGA and MNIST we don't care as it is only data loading. 
